import jieba  
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

txt = open("新文字文件.txt", encoding="utf-8").read()
#加載停用詞表   
words  = jieba.lcut(txt)  
new = {} 
for word in words:   
     new[word] = new.get(word,0) + 1 
exc={"。","，","~","\n","="," ",":","!","?","？","！",",","...","\t"}
for word in exc:
    del new[word]

items = list(new.items())  
items.sort(key=lambda x:x[1], reverse=True) 
for i in range(100):   
    print (items[i])
